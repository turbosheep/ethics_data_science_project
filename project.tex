\documentclass[12pt]{article}
\usepackage{mathptmx}
%\usepackage{kantlipsum}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\singlespacing\small}

\usepackage[style=apa, 
            language=american, 
            backend=biber, 
            natbib=true, 
            hyperref=true, 
            uniquelist=true]{biblatex}

\DeclareLanguageMapping{american}{american-apa}
\addbibresource{proposal_bibliography.bib}

\title{Detecting ``Fake News" on Facebook}
\author{Hannah Eyre \and Zane Zakraisek}
\begin{document}
\maketitle
\setstretch{1.2}

The term ``Fake News" gained popularity during the United States' 2016 Presidential Election to describe a rapidly spreading phenomena of news articles deliberately spreading false information and hoaxes, often through attention grabbing headlines or headlines that resemble legitimate sources \citep{guardian}. It became particularly notorious on social media sites and Facebook in particular. Here, the top 20 articles from fake news sites and hyperpartisan blogs garnered more user interaction between August 1st and election day on November 8th than the top 20 articles from a variety of established news sources. These included {\it The New York Times}, {\it Washington Post}, {\it Business Insider}, and Fox News \citep{buzzfeed}.

\textit{FactCheck.org}, part of the Annberg Public Policy Center at the University of Pennsylvania, breaks down how an individual can identify fake news into eight parts \citep{factcheck}:
\begin{quote}
\begin{enumerate}
\item Consider whether the source is credible.
\item Read beyond the headline.
\item Check whether the author is credible or real.
\item Check whether the article is recent.
\item Check whether the article is a joke or satire.
\item Consider your own biases and how they affect your judgment.
\item Check supporting sources (if any) and make sure they abide by the same rules.
\item Ask experts or fact-checking sites.
\end{enumerate}
\end{quote}

Rapidly spreading fake news articles have a range of consequences, one instance culminating in a gunman attacking a Washington, DC pizza parlor over allegations of a satanic child sex abuse ring centered around John Podesta, Hillary Clinton's 2016 campaign manager, in a conspiracy called ``Pizzagate" \citep{pizzagate}. Various politicians and government agencies in the United States and internationally have voiced opinions on what qualifies as fake news, and more importantly, what to do about it. However, up to this point, no consensus has been reached. Facebook was initially reluctant to admit there was any problem with fake news on their website, however, Facebook's CEO Mark Zuckerberg has since released a statement describing how they plan to deal with fake news in the future, including renaming the phenomenon ``false news" \citep{zuckerberg}.

As an increasingly global and hotly contested issue, we would like to to explore what responsibility Facebook has in regards to these eight points. We will discuss whether they have a responsibility to develop tools to detect fake news based off these guidelines and, if these tools exist, whether they should be used to remove content from the site. We will be using two particular examples, Pizzagate as mentioned above and conspiracies spread in the hours and days following the Las Vegas shooting. We feel that Pizzagate is a primary example of what happens when fake news reaches a critical mass of interest online, having large followings on Reddit, 4chan, Breitbart and Infowars that actively encouraged ``citizen investigation". Additionally, the Las Vegas shooting conspiracies are a group of examples of the rapid spreading of fake news, particularly when real information is unavailable or unclear. In the early morning hours following the October 1, 2017 shooting, the website 4chan organized a successful effort to manipulate trending topics across the web and spread misinformation before real information was distributed.

The other primary focus of our analyzation comes from a utilitarian perpective. When analyzing any issue, it's important to define what framework you're working in, along with what premises your arguments are based on. During our analysis of the Facebook's role in moderating fake news, we'll be working under a utilitarian framework. In particular, our primary goal is reducing the negative utility fake news brings about. As we examine role that fake news has played in the events mentioned, it is evident that fake news results in a negative utility for Facebook's users. News on Facebook has the potential to result in positive utility, and like fake news, it is due to the rapid spread and wide range of people reached. We believe access to a variety of news sources is an important part of interacting with social media and results in a greater positive utility than fake news causes negative. Because of this, we believe removing news in general from Facebook would increase negative utility overall. As a result, we find that it is far more important to try to minimize the corresponding negative utility caused by fake news.

As we address each of these eight points mentioned above, the format shall be as follows. We'll first examine the status quo where Facebook has not publicly addressed efforts made to combat fake news and how that is affecting users. Next, we'll examine the outcome if Facebook chose to implement a fake news monitoring system using this technique. Lastly, we'll weigh the different utilities and select the one that we feel minimizes the negative utility. In addition to this, we'll also be examining whether or not the method is technically feasible to implement with current technology.

\section{Credible sources: Sites and Authors}
The first method in combating the spreading of fake news is to check whether or not the article in question comes from a credible source. This involves not just the author, but the site as well. To start off, we'll examine the utility if Facebook chose \textit{not to} implement this source credibility check. Although Facebook implements a very light form of censorship (citep), the scenario would be quite similar to how Facebook currently monitors articles, which is very little. While most articles that Facebook news feeds show users are based off topics that they've shown interest in, nearly every Facebook user has probably been shown an article from an underground website that someone on their friends list has liked or shared. If Facebook didn't implement source credibility prioritization, then the effect of one person sharing an article like this could have quite a negative outcome. The article in question can easily make its way to hundreds of people. In most cases, this has the effect of increasing the amount of negative utility, as most fake news articles are spread with negative motivations in mind.

Now assume Facebook was to implement source credibility prioritization. In this same scenario, if someone was to like or share an article, Facebook could first check its feedback for that particular site and/or author. If there is enough feedback, and the score is negative, Facebook could deprioritize the article on other people's news feeds by a factor of the magnitude of the negative score. Likewise, if the site or author has a positive score, Facebook could choose to display the article as it normally would, or maybe even prioritize it higher. From a utility perspective, this has the potential to significantly reduce negative utility brought about by fake sites. On the other hand, there does exist the potential to filter out some sites which many people may find to be perfectly acceptable, which contributes to false positives. In other words, this has the potential to significantly decrease negative utility, with a smaller side effect of decreasing some positive utility. From an overall perspective however, implementing source credibility prioritization has the net effect of decreasing negative utility.

In terms of the technical feasibility of this point, we that it is one of the lowest hanging fruits in the pursuit of diminishing the spread of fake news. Once possibility would involve Facebook storing a few metrics for a site based upon feedback received by readers. Facebook already has a way to flag particular articles as inappropriate. In the same way, Facebook could allow a reader to optionally rate the credibility of the article or the site in general, and use these ratings to prioritize and deprioritize authors and sites respectively. 

Lastly, there is the idea of checking whether or not the author of an article is indeed a real person, and is indeed who they claim to be. We feel that this would be a much more difficult task for Facebook to implement. Additionally, while author authentication is a concept that would indeed help lower negative utility, the implementation of it would most likely cross the privacy line. This in turn increases negative utility. For example, Facebook may attempt to link the author of an article on some site to an individual account on Facebook for an added level of author verification. Since this has implications in terms of privacy, we feel that author authentication is not an angle that Facebook should approach this problem from. (hannah- what about the "verified user" thing that already exists on facebook, could that be used? make verification for that checkmark more stringent or up to some determined "fake news standards", then only allow topics from verified accounts onto trending? has problems with viral user posts though)

\section{Reading Beyond the Headline}

When posting an external link to Facebook, information is drawn from the content inside \texttt{meta} elements of the page source by Facebook's web crawler and placed into a preview on the post itself. If an element labeled according to the Open Graph protocol, the crawler will place it in the link preview on the site \citep{fbwebmaster}. With this system in place, every linked website will appear the same on a users news feed. Any site can determine how its posts show up when linked on Facebook, regardless of the quality of the content or site itself. \\

\begin{figure}[h]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[scale=.3]{pizzagate_alex_jones_fb}
	\end{minipage}
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[scale=.3]{pizzagate_npr_fb}
	\end{minipage}
	\caption{A comparison of posts by Alex Jones, creator of {\it Infowars}, \citep{alex_jones_pizzagate_post} and {\it NPR} \citep{npr_pizzagate_post} about Pizzagate.}
\end{figure}

This uniform formatting treats {\it NPR} and {\it Infowars} or any other site that uses the \texttt{meta} elements according to Facebook's specifications as equals. It places less emphasis on the source of the article than on the headline and picture. The examples above are posted from official accounts, allowing users to easily see where the article comes from. However, the same articles posted by friends or unofficial pages make the source more difficult to determine. In addition, 50\% of shared links generate less than 1\% of clicks on social media \citep{clicks_vs_shares}, meaning most shares happen without the user clicking the link and reading the article. Therefore what a user sees on Facebook's preview is most likely all that user sees and is entirely determined by the web developer, with the possibility that the headline, picture, and description are totally unrelated to the actual content of the article, factually incorrect, or telling an attention-grabbing but incomplete story.

If Facebook were to put more focus on the content of the article rather than the title, this would reduce the negative utility generated by misleading or inaccurate articles being shared based on preview content that is entirely separate from what is inside the link itself. Automatic summarization is a potential solution, where an automatically generated summary is added to the preview, possibly in place of the description section created by the author. This forces the text shown in the preview to be more representative of the actual content the user would read if they were to click on the article. An alternative would be to take the contents of the article itself and place a truncated version into the preview content on a user's feed. This would be easily understood by users with a short explanation, and potentially aid in users understanding the contents of the article before they share, minimizing the impact and negative utility generated by misleading headlines.

Facebook could resolve this by implementing an automatic summarization of the contents of the page, however summarizing text has many technical hurdles. Automated summarization of text is an active area of research in natural language processing and even evaluating the quality of summaries in relation to longer pieces of text is often unclear. Summarizing often requires context for concepts, objects, or people mentioned and a system summarizing based off only the text presented to it can write summaries as misleading as what would have been in it's place without the automatic system. The truncated text could be subject to loopholes when gathering the text and potentially rife with abuse. Web developers could easily put misleading descriptions in text that is invisible on the page or only load text content from a server rather than being embedded in the page for Facebook's web crawler.

\section{Age of the Article}

Situations like the Las Vegas shooting pose a difficult challenge for Facebook. In chaotic situations, important information can be gained from an individual's posts online, particularly before traditional media arrives. However it is not clear until much later which posts are accurate and which are not. In the instances where the posts were not true, included pictures were often of lesser known or regionally unknown celebrities, such as foreign soccer players. Fake posts of this type will increase negative utility for people trying to help locate missing people. However, there exists the possibility that limiting or preventing posts of this type might create more negative utility from angry and concerned friends and family. One possibility to combat both these issues could be limiting blog posts and unverified news sources from reaching the trending topics list until a certain time after the event occurs. This could be a way to minimize negative utility for users looking for information on the incident.

Facebook has recently implemented the ability for users to ``check in" safe after a variety of events from around the world. The primary motivation for this is an attempt to minimize the flood of fake posts searching for friends and family without impeding anyone from actually making the posts. However, the effectiveness of this system is not known. Anyone who does not hear from someone they care about may still post in search of that person, leaving the door open for fake posts to flood a user's feed. Limiting content on the trending topics list immediately surrounding to verified accounts is very feasible for Facebook. With this said however, making a verified account status more difficult to receive would be necessary or maybe a different verification process for news stories or media companies would need to be developed. One potential would be to only allow trending topics from news sources with a high percentage of their content being previously verified. This could allow networks with a history of good behavior to get their stories to be seen without any interaction with developing stories themselves.

While an enormous amount of negative utility is generated on Facebook every time an incident like the Las Vegas shooting happens, determining what is real and what is fake is not clear, even to potential human curators, let alone algorithmic curators. The negative utility earned from implementing a bad classifier for filtering personal user's posts could easily generate far more negative utility than was initially created by the fake posts themselves. Fake or unverified news stories reaching trending lists might be easier to control with less potential negative utility generated from mistakes as long as Facebook develops a method of curation that filters out unverifiable sources, such as message forums and blog posts. 

While an enormous amount of negative utility is generated on Facebook every time an incident like the Las Vegas shooting happens, determining what is real and what is fake is not clear, even to potential human curators, let alone algorithmic curators. The negative utility earned from implementing a bad classifier for filtering personal user's posts could easily generate far more negative utility than was initially created by the fake posts themselves and should not immediately be considered as a method for countering fake news. Fake or unverified news stories reaching trending lists should be a method used, because it will be easier to control with less potential negative utility generated from mistakes as long as Facebook develops a method of curation that filters out unverifiable sources, such as message forums and blog posts. 

\section{Article Genre}

Humorous content is popular on Facebook ranging from meme pages, to video skits, to satire news pages. The satirical media company, \textit{The Onion}, has 6.7 million likes on Facebook, 700,000 more likes than \textit{The Washington Post}. \textit{The Onion} is a well known satire site and, upon first inspection, makes itself clear that it is always satire. However, \textit{The Onion} as well as it's sister site \textit{Clickhole} have a history of being mistaken for real news. If one of the most well known online satire websites has a notorious history for making headlines in national news when even politicians, such as Former White House Press Secretary Sean Spicer, took it seriously \citep{spicey}. Other links from less well known sources will cause even more confusion, although the reach of individual articles by less popular websites may smaller than that of \textit{The Onion}. There is some negative utility gained by users confusing satirical articles for real ones. Publications like \textit{The New Yorker} can be more problematic because it is not solely a satire site, and simply publishes some satire, often without clear demarcation between what is satire and what is not. If something as labeled or appears like satire, it is not necessarily immune from being harmful. The conspiracies spread in the aftermath of the Las Vegas shooting were spread primarily by a message board on the website 4chan called /pol/. /pol/, short for ``Politically Incorrect," brands itself as satire and frequently excuses it's own bad behavior as a joke or satire.

If Facebook were to mark articles it detects as satire somewhere on the news feed, it may reduce negative utility for users who would be confused by the article while for users who enjoy the content, it would have minimal impact. Filtering satire altogether would create far more negative utility for users who enjoy the content than often brief spells of confusion caused upon initially seeing a satirical article. Allowing all satirical content onto the site, however, will also contribute negative utility.

A recent paper using satire to detect misleading news achieved 90\% precision and 84\% recall with a relatively simple model \citep{satire_detection}. This shows that, with further research, this could be a promising method of aiding users in determining whether news is misleading. However, determining whether an article is satire may not show the complete picture. Humor can still be malicious and simply classifying an article as satire may let some things that could be harassing, bigoted, or inciting violence onto a user's news feed when it should have been caught by another filter.

Detecting satire is a rapidly improving area in natural language processing, however classifying text as satirical or not does not paint the entire picture and should not be a determining factor in whether an article is removed from Facebook or prevented from reaching a trending topics list. However, marking an article as satire might be a helpful tool if provided to users for when they consider whether an article is true.


\section{Interpretation Bias}
Facebook recently came under fire for their manual curation of trending news topics when former Facebook employees were interviewed by Gizmodo about the trending news section. One curator said ``There was no real standard for measuring what qualified as news and what didn't" and that they regularly avoided right wing sites such as \textit{Breitbart} and \textit{The Blaze}. When pressed for a comment, Facebook responded by claiming they ``take allegations of bias very seriously" \citep{gizmodo_fb_news_curation}. By framing bias as a negative, Facebook implies that they view themselves as a neutral entity and that they believe it is possible for the website and trending topics to be free of bias. Every stance on certain types of news generates negative utility for those who wish to see it and do not, and those who wish to not see it but do. For these users, the site is not working as optimally as it could. However, a neutral stance on fake news has other negative utility generated by the articles themselves. Many fake news articles have consequences that reach beyond just an individual user's news feed. The gunman attacking Comet Ping Pong in response to Pizzagate is an example of this. The conspiracy did not begin and end online, it involved with months of harassment and threats to employees and patrons of Comet Ping Pong and ended with the shooting that could have easily ended with multiple people who may not even use Facebook injured or dead.

There will always be negative utility generated by users who want to see the types of content Facebook disallows, however Facebook can minimize the negative utility generated by the types of content with consequences outside of Facebook itself. In order to do so, Facebook would need to decide what kinds of content are allowed on the site and be able to accept that any stance will be subject to criticism. For Facebook to consider what biases their employees and their platform bring to news curation, they first need to accept that they will always have bias. Bias is an unavoidable part of curating news, by choosing what to allow through content filters, even by banning things commonly banned in other forms of social media, such as pornography, is a kind of bias.

Facebook would have to first have an internal discussion among employees and shareholders of the company, to help narrow down what kind of platform they would like to have for users. The company itself would have to come up with a value statement and be able to stand by those beliefs in the face of backlash or be open to finding a way to work with users and advertisers without compromising those values. This would be difficult, Facebook would likely need to prepare to protect employees such as community managers from harassment. However examining what the company values as a whole is a crucial first step in deciding how to move forward on any other part of implementing a fake news moderation system.

\section{Cross Referencing}
The next method for assisting in classifying fake news on Facebook is to implement fact checking for an article. This involves using a web crawler to go around and potentially fact check that sites. More precisely, in an ideal scenario, when a user posts a link on Facebook, the page linked to should have been indexed, and there should be a metric associated with the page that conveys the trustworthiness of the content presented. If the score falls below a particular threshold, Facebook may deprioritize the link.  

From a utilitarian perspective, we feel that on the surface, this method seems like it has the potential to greatly hinder the spread of fake news. If the content on a page is primary false, then deprioritizing it hinders it getting around the web. The major downside we see is that often, the articles linked to on Facebook are not formally written scholarly articles, but rather more casual ones. In these types of articles, opinions are presented as facts, and vice-versa. This creates an ambiguity in how we define a fact. We feel that in this case, even if there existed an natural language processing oracle that could perfectly distinguish fact from opinion, the majority of articles would be flagged as false negative simply because of people's writing style. Overall, what little positive utility that this brings would be quickly overshadowed by negative utility.

In terms of the technical feasibility, the same argument can be made here. From an NLP perspective, it would be challenging enough to even write a method that could parse the text and pull out statements, let alone distinguish fact from opinion. Additionally, the context of the statement would need to go through sentiment analysis to determine the authors intentions for even writing the comment in the first place. For this reason, we feel that Facebook attempting to distinguish fact from opinion is technically infeasible, and because of this, would result in in an increase in the negative utility.

\section{Supporting sources}
The final method for identifying fake news is to check supporting sources (if any) and make sure they abide by the same rules. In other words, this method attacks the problem with a recursive approach. When a user wants to share an article on the site, the fake news classification algorithm could take a depth first approach at calculating a trustworthiness metric. This can be done by first calculating the trustworthiest of the sources linked to in the article, and then using this metric in the overall trustworthiness calculation for the main article.

From a utilitarian perspective, where our main goal is to minimize the amount of negative utility brought about by fake articles, we feel that implementing this method has a few angles to look at the problem from. The core idea of this method is to first apply the other methods that we feel are appropriate. We next take a depth first approach, using these metrics to generate a metric for the primary article. We feel that for the most part, this would be an extra step in minimizing the spread of fake news. One point to keep in mind however is that by taking a recursive approach, we may be rating the credibility of one page based on the credibility of a page with multiple degrees of separation that have virtually nothing to do with each other. Also, as mentioned in \textit{Credible sites and Sources}, there doesn't exist a reliable algorithm to to flag a link with the context that it's being cited in. For example, if an author is citing a fake news article with the intention of pointing out the fake news, sentiment analysis would need to be implemented so that the trustworthiness metric would somehow understand this context, and not use this against the articles score.

In terms of the technical feasibility of the implementation, there are a few issues that would make this point difficult to implement. The first hurdle is that there would have to be some kind of standardization to the method that sites use to track citations and references. Most sites that people read have a nonstandard citations section at the bottom of the page that consists of random bits of metadata and URLs. Although this conveys the required information to the \textit{reader}, it fails to be parsable by a web crawler. Additionally, this depth first approach to calculating source credibility would involve storing a large amount of metadata.

\section{Conclusion}

With the increasing veracity of fake news across all of social media, particularly Facebook, and the consequences of letting misinformation spread rapidly and unchallenged, our utilitarian analysis has shown that Facebook has an ethical responsibility to come up with an effective and equitable strategy to combat its spread. Additionally, a Pew Research Center survey found 68\% of Facebook users get news from Facebook, but only 33\% of Facebook users get news from local TV or dedicated news sites and apps \citep{pew_news}. Facebook earns more than twice the engagement in news than any single non-social media platform, but Facebook does not hold itself to any standard for the integrity of their sources unlike most traditional formats. Because of its large impact on users, Facebook has an ethical obliation to attempt to stop the spreading of this fake news among its users.

As we've seen in our analysis of the methods of inhibiting fake news, there are many challenges involved in implementing these methods. Not only are some of these methods technically infeasible, but implementing them in algorithm form may open up the doors to more negative utility than it actually stops. Additionally, with the volume of posts shared, it is not possible Facebook to tackle these problems manually with human intervention. Algorithmic methods seem to be the most practical solution if human moderation and automatic moderation were equally accurate. Like we've seen however, there are just as many different struggles for developing an algorithmic fake news detection system. The primary issues that we see arising come down to privacy concerns, interpreting an author's motivations, and automatic fact checking. With this said however, we feel like (WHICH ONES WE FEEL ARE WORTH IMPLEMENTING)

If Facebook were to decide to invest in an automated system for detecting fake news, the most important aspect in our opinion is transparency at every step of the process. In order to begin designing such a system, Facebook needs to determine what they value in a news source and be willing to stand by those beliefs in the face of adversity or openly change them in response to criticism. By not filtering sites and authors that promote racism, fake news, harassment campaigns and so on, Facebook is making a political statement that it will not contribute to the spread of sites that have a goal of hurting individuals. Although there are grey areas in the process of filtering news, the most important component in the matter is the transparency. As soon as Facebook chooses to not release its classification methods, it provides individuals with an excuse to point fingers at Facebook for bias. 

\newpage
%\bibliographystyle{apalike}
%\bibliography{proposal_bibliography.bib}
\printbibliography

\end{document}
